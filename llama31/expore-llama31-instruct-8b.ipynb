{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84c693e-54f7-432f-8c1a-81818039a21c",
   "metadata": {},
   "source": [
    "# [Understand How Llama3.1 Works — A Deep Dive Into the Model Flow](https://medium.com/@yuxiaojian/understand-how-llama3-1-works-a-deep-dive-into-the-model-flow-b149aba04bed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8172ab85-50d7-496b-b044-d918613279e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade transformers\n",
    "#%pip install --upgrade trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b37ec71-ea37-465c-a8a2-5fd61443b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel,PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eecca0fe-a6fe-46a4-9831-77cddd2fa189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "#Configure Huggingface token if download model from Huggingface\n",
    "import getpass\n",
    "import os\n",
    "# Get the Huggingface Token\n",
    "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ecc823e-8c5e-4991-8fe3-e27c26875aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from HF \n",
    "base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835f235f-77b9-4ee3-8944-ee8a393333da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e79fdd03d74d43aeafe3567c8f4653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Quantize the llama3.1 FP16 model to BNB NF4. Load the quantized model to GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Use the code below to load directly from HF\n",
    "base_model_bnb_4b = AutoModelForCausalLM.from_pretrained(base_model, device_map=\"cuda:0\", quantization_config=bnb_config, token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af0c020-2bd2-49d2-b0e0-1151b60c1d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128256\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8976bafb-c1ec-4fd2-8feb-5d5e8ed49757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<|begin_of_text|>', 128000), ('<|start_header_id|>', 128006), ('system', 9125), ('<|end_header_id|>', 128007), ('ĊĊ', 271), ('Based', 29815), ('Ġon', 389), ('Ġthe', 279), ('Ġinformation', 2038), ('Ġprovided', 3984), (',', 11), ('Ġrewrite', 18622), ('Ġthe', 279), ('Ġsentence', 11914), ('Ġby', 555), ('Ġchanging', 10223), ('Ġits', 1202), ('Ġtense', 43787), ('Ġfrom', 505), ('Ġpast', 3347), ('Ġto', 311), ('Ġfuture', 3938), ('.', 13), ('<|eot_id|>', 128009), ('<|start_header_id|>', 128006), ('user', 882), ('<|end_header_id|>', 128007), ('ĊĊ', 271), ('She', 8100), ('Ġplayed', 6476), ('Ġthe', 279), ('Ġpiano', 27374), ('Ġbeautifully', 32719), ('Ġfor', 369), ('Ġhours', 4207), ('Ġand', 323), ('Ġthen', 1243), ('Ġstopped', 10717), ('Ġas', 439), ('Ġit', 433), ('Ġwas', 574), ('Ġmidnight', 33433), ('.', 13), ('<|eot_id|>', 128009), ('<|start_header_id|>', 128006), ('assistant', 78191), ('<|end_header_id|>', 128007), ('ĊĊ', 271)]\n"
     ]
    }
   ],
   "source": [
    "# Get a token and ID mapping\n",
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))\n",
    "\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Based on the information provided, rewrite the sentence by changing its tense from past to future.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "She played the piano beautifully for hours and then stopped as it was midnight.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "print_tokens_with_ids(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c67d87-0d0f-456c-b9dd-a3e054bff9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
       "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
       "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
       "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
       "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
       "         128006,  78191, 128007,    271]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the token IDs\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "934feea3-5eea-418a-91dd-f8d3e9f45b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
       "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
       "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
       "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
       "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
       "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,  59624,\n",
       "            304,    279,   3938,  43787,   1473,   8100,    690,   1514,    279,\n",
       "          27374,  32719,    369,   4207,    323,   1243,   3009,    439,    433,\n",
       "            374,  33433,     13, 128009]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After model\n",
    "outputs = base_model_bnb_4b.generate(input_ids=input_ids,\n",
    "                          pad_token_id=tokenizer.eos_token_id,\n",
    "                          max_new_tokens=200,\n",
    "                          do_sample=True,\n",
    "                          top_p=0.9,\n",
    "                          temperature=0.1)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd447815-2364-4f12-85cd-2bc7e4c80da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Based on the information provided, rewrite the sentence by changing its tense from past to future.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "She played the piano beautifully for hours and then stopped as it was midnight.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here is the sentence rewritten in the future tense:\n",
      "\n",
      "She will play the piano beautifully for hours and then stop as it is midnight.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Map the IDs back to tokens\n",
    "result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e614fa9a-0382-4841-9ef3-85b0d9e608aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": false,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 8.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_bnb_4b.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d993c8-29f4-4db0-adc4-9e3a3d982a29",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"image/llama31-arch.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b850cbd-1250-4314-9368-13b3bba90e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_bnb_4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d0cb1e0-ac6b-43c3-946c-14d5daa8d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Helper functions for Rotary Position Embedding (RoPE) and grouped-query attention\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    \"\"\"\n",
    "    Apply rotary positional embeddings to query and key tensors.\n",
    "    \n",
    "    Args:\n",
    "    q, k: Query and key tensors\n",
    "    cos, sin: Cosine and sine components of rotary embeddings\n",
    "    position_ids: Tensor of position IDs\n",
    "    \n",
    "    Returns:\n",
    "    q_embed, k_embed: Query and key tensors with rotary embeddings applied\n",
    "    \"\"\"\n",
    "    # Reshape cosine and sine tensors\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    \n",
    "    # Select relevant positional embeddings\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    \n",
    "    # Apply rotary embeddings\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"\n",
    "    Rotate half of the hidden dimensions of the input tensor.\n",
    "    \n",
    "    This operation is part of the rotary position embedding technique.\n",
    "    \n",
    "    Args:\n",
    "    x: Input tensor\n",
    "    \n",
    "    Returns:\n",
    "    Tensor with half of its last dimension rotated\n",
    "    \"\"\"\n",
    "    x1 = x[..., :x.shape[-1] // 2]  # First half of hidden dims\n",
    "    x2 = x[..., x.shape[-1] // 2:]  # Second half of hidden dims\n",
    "    return torch.cat((-x2, x1), dim=-1)  # Concatenate rotated halves\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Repeat key and value states for grouped-query attention.\n",
    "    \n",
    "    This function expands the key and value states to match the number of query heads\n",
    "    in grouped-query attention mechanisms.\n",
    "    \n",
    "    Args:\n",
    "    hidden_states: Input tensor of shape (batch, num_key_value_heads, seqlen, head_dim)\n",
    "    n_rep: Number of repetitions (usually num_query_heads // num_key_value_heads)\n",
    "    \n",
    "    Returns:\n",
    "    Tensor of shape (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states  # No need to repeat if n_rep is 1\n",
    "    \n",
    "    # Expand and reshape to repeat the key/value states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c481e-e85c-4d32-bc7f-18b78321d190",
   "metadata": {},
   "source": [
    "## Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "611d02ae-4209-4ff0-8bad-d303955ad635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([1, 49, 4096])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "position_ids = torch.arange(0, input_ids.shape[1]).unsqueeze(0).cuda()\n",
    "seq_length = input_ids.shape[1]\n",
    "embeddings = base_model_bnb_4b.model.embed_tokens(input_ids)\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83c7c1ad-56f3-42b8-8e34-d6a94679daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = embeddings\n",
    "hidden_size = base_model_bnb_4b.config.hidden_size\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bd7b6eb-5300-4cca-a63d-58803e1b5ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer 0\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 1\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 2\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 3\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 4\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 5\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 6\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 7\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 8\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 9\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 10\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 11\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 12\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 13\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 14\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 15\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 16\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 17\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 18\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 19\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 20\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 21\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 22\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 23\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 24\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 25\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 26\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 27\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 28\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 29\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 30\n",
      "  Output shape: torch.Size([1, 49, 4096])\n",
      "Processing layer 31\n",
      "  Output shape: torch.Size([1, 49, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Process through each layer of the model\n",
    "for layer_idx, layer in enumerate(base_model_bnb_4b.model.layers):\n",
    "    print(f\"Processing layer {layer_idx}\")\n",
    "    \n",
    "    # 1. Input LayerNorm\n",
    "    normalized_hidden_states = layer.input_layernorm(hidden_states)\n",
    "    \n",
    "    # 2. Self-attention mechanism\n",
    "    # 2.1 Query, Key, Value projections\n",
    "    query_states = layer.self_attn.q_proj(normalized_hidden_states)\n",
    "    key_states = layer.self_attn.k_proj(normalized_hidden_states)\n",
    "    value_states = layer.self_attn.v_proj(normalized_hidden_states)\n",
    "    \n",
    "    # 2.2 Reshape and transpose Q, K, V\n",
    "    # (batch_size, seq_length, num_heads, head_dim) -> (batch_size, num_heads, seq_length, head_dim)\n",
    "    query_states = query_states.view(batch_size, seq_length, layer.self_attn.num_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(batch_size, seq_length, layer.self_attn.num_key_value_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(batch_size, seq_length, layer.self_attn.num_key_value_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "    \n",
    "    # 2.3 Apply rotary positional embeddings\n",
    "    kv_seq_len = key_states.shape[-2]\n",
    "    cos, sin = layer.self_attn.rotary_emb(value_states, position_ids)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "    \n",
    "    # 2.4 Handle grouped-query attention\n",
    "    # Repeat K and V for each query group\n",
    "    key_states = repeat_kv(key_states, layer.self_attn.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, layer.self_attn.num_key_value_groups)\n",
    "    \n",
    "    # 2.5 Compute attention scores\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(layer.self_attn.head_dim)\n",
    "    attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "    \n",
    "    # 2.6 Apply attention to values\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    \n",
    "    # 2.7 Reshape attention output\n",
    "    # (batch_size, num_heads, seq_length, head_dim) -> (batch_size, seq_length, hidden_size)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, hidden_size)\n",
    "    \n",
    "    # 2.8 Output projection\n",
    "    attn_output = layer.self_attn.o_proj(attn_output)\n",
    "    \n",
    "    # 2.9 Residual connection\n",
    "    hidden_states = hidden_states + attn_output\n",
    "    \n",
    "    # 3. Post-attention LayerNorm\n",
    "    normalized_hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "    \n",
    "    # 4. MLP (Feed-Forward Network)\n",
    "    # 4.1 Apply gate and up projections\n",
    "    gate_output = layer.mlp.gate_proj(normalized_hidden_states)\n",
    "    up_output = layer.mlp.up_proj(normalized_hidden_states)\n",
    "    \n",
    "    # 4.2 Apply activation function and element-wise multiplication\n",
    "    mlp_output = up_output * layer.mlp.act_fn(gate_output)\n",
    "    \n",
    "    # 4.3 Down projection\n",
    "    mlp_output = layer.mlp.down_proj(mlp_output)\n",
    "    \n",
    "    # 4.4 Residual connection\n",
    "    hidden_states = hidden_states + mlp_output\n",
    "    \n",
    "    print(f\"  Output shape: {hidden_states.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb3e5097-cf7b-4106-bc2a-54a3182f2b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271]], device='cuda:0')\n",
      "Most likely next token ID: 8586\n"
     ]
    }
   ],
   "source": [
    "# Final LayerNorm\n",
    "# Normalize the hidden states from the last layer\n",
    "hidden_states = base_model_bnb_4b.model.norm(hidden_states)\n",
    "\n",
    "# Language Model Head\n",
    "# Project the normalized hidden states to the vocabulary space\n",
    "lm_logits = base_model_bnb_4b.lm_head(hidden_states)\n",
    "\n",
    "# Get the logits for the last token\n",
    "# We're only interested in predicting the next token, so we take the last position\n",
    "last_token_logits = lm_logits[:, -1, :]\n",
    "\n",
    "# Note: The following line is commented out as we're using a more sophisticated sampling method\n",
    "# next_token = torch.argmax(last_token_logits, dim=-1)\n",
    "\n",
    "# Step 7: Apply temperature\n",
    "# Temperature adjusts the randomness of predictions. Lower values make the model more confident.\n",
    "temperature = 0.1\n",
    "scaled_logits = last_token_logits / temperature\n",
    "\n",
    "# Step 8: Apply top-p (nucleus) sampling\n",
    "# This method truncates the least likely tokens whose cumulative probability exceeds (1 - top_p)\n",
    "top_p = 0.9\n",
    "# Sort logits in descending order\n",
    "sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
    "# Calculate cumulative probabilities\n",
    "cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "# Remove tokens with cumulative probability above the threshold\n",
    "sorted_indices_to_remove = cumulative_probs > top_p\n",
    "# Shift the indices to keep also the first token above the threshold\n",
    "sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "sorted_indices_to_remove[..., 0] = 0\n",
    "# Scatter sorted tensors to original indexing\n",
    "indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "# Set logits of removed indices to negative infinity\n",
    "scaled_logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "# Step 9: Sample from the filtered distribution\n",
    "# Convert logits to probabilities\n",
    "probs = torch.softmax(scaled_logits, dim=-1)\n",
    "# Randomly sample a token based on the calculated probabilities\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "# Print the input tokens and the predicted next token\n",
    "print(f\"\\nInput tokens: {input_ids}\")\n",
    "print(f\"Most likely next token ID: {next_token.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275b4b6-2d1a-4cd8-ac7c-91d42cfaab5f",
   "metadata": {},
   "source": [
    "## Create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8291fd11-0d83-494e-a438-b86e47e1fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, top_k=50, top_p=0.95):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Prepare input\n",
    "        position_ids = torch.arange(0, input_ids.shape[1]).unsqueeze(0).cuda()\n",
    "        seq_length = input_ids.shape[1]\n",
    "        batch_size = 1\n",
    "        hidden_size = model.config.hidden_size\n",
    "\n",
    "        # Token embedding\n",
    "        hidden_states = model.model.embed_tokens(input_ids)\n",
    "\n",
    "        # Process through each layer\n",
    "        for layer in model.model.layers:\n",
    "            # 1. Input LayerNorm\n",
    "            normalized_hidden_states = layer.input_layernorm(hidden_states)\n",
    "            \n",
    "            # 2. Self-attention\n",
    "            query_states = layer.self_attn.q_proj(normalized_hidden_states)\n",
    "            key_states = layer.self_attn.k_proj(normalized_hidden_states)\n",
    "            value_states = layer.self_attn.v_proj(normalized_hidden_states)\n",
    "            \n",
    "            # Reshape and transpose\n",
    "            query_states = query_states.view(batch_size, seq_length, layer.self_attn.num_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "            key_states = key_states.view(batch_size, seq_length, layer.self_attn.num_key_value_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "            value_states = value_states.view(batch_size, seq_length, layer.self_attn.num_key_value_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "            \n",
    "            # Apply rotary embeddings\n",
    "            cos, sin = layer.self_attn.rotary_emb(value_states, position_ids)\n",
    "            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "            \n",
    "            # Handle grouped-query attention\n",
    "            key_states = repeat_kv(key_states, layer.self_attn.num_key_value_groups)\n",
    "            value_states = repeat_kv(value_states, layer.self_attn.num_key_value_groups)\n",
    "            \n",
    "            # Compute attention scores and apply attention\n",
    "            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(layer.self_attn.head_dim)\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_states)\n",
    "            \n",
    "            # Reshape attention output\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, hidden_size)\n",
    "            \n",
    "            # Output projection\n",
    "            attn_output = layer.self_attn.o_proj(attn_output)\n",
    "            \n",
    "            # Residual connection\n",
    "            hidden_states = hidden_states + attn_output\n",
    "            \n",
    "            # 3. Post-attention LayerNorm\n",
    "            normalized_hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "            \n",
    "            # 4. MLP\n",
    "            mlp_output = layer.mlp.gate_proj(normalized_hidden_states)\n",
    "            mlp_output = layer.mlp.up_proj(normalized_hidden_states) * layer.mlp.act_fn(mlp_output)\n",
    "            mlp_output = layer.mlp.down_proj(mlp_output)\n",
    "            \n",
    "            # Residual connection\n",
    "            hidden_states = hidden_states + mlp_output\n",
    "\n",
    "        # Final LayerNorm\n",
    "        hidden_states = model.model.norm(hidden_states)\n",
    "\n",
    "        # Language Model Head\n",
    "        lm_logits = model.lm_head(hidden_states)\n",
    "\n",
    "        # Get the logits for the last token\n",
    "        last_token_logits = lm_logits[:, -1, :]\n",
    "\n",
    "\n",
    "        # Step 7: Apply temperature\n",
    "        # Temperature adjusts the randomness of predictions. Lower values make the model more confident.\n",
    "        scaled_logits = last_token_logits / temperature\n",
    "        \n",
    "        # Step 8: Apply top-p (nucleus) sampling\n",
    "        # This method truncates the least likely tokens whose cumulative probability exceeds (1 - top_p)\n",
    "        # Sort logits in descending order\n",
    "        sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
    "        # Calculate cumulative probabilities\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        # Scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        # Set logits of removed indices to negative infinity\n",
    "        scaled_logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Step 9: Sample from the filtered distribution\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.softmax(scaled_logits, dim=-1)\n",
    "        # Randomly sample a token based on the calculated probabilities\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Print the input tokens and the predicted next token\n",
    "        print(f\"\\nInput tokens: {input_ids}\")\n",
    "        print(f\"Most likely next token ID: {next_token.item()}\")\n",
    "\n",
    "        # Append the new token to input_ids\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        \n",
    "        # Check if we've generated an end-of-sequence token\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7afc24a4-69b1-410c-9bea-ac4c07c0cbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271]], device='cuda:0')\n",
      "Most likely next token ID: 8586\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586]], device='cuda:0')\n",
      "Most likely next token ID: 374\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374]], device='cuda:0')\n",
      "Most likely next token ID: 279\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279]],\n",
      "       device='cuda:0')\n",
      "Most likely next token ID: 11914\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914]],\n",
      "       device='cuda:0')\n",
      "Most likely next token ID: 449\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449]],\n",
      "       device='cuda:0')\n",
      "Most likely next token ID: 1202\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202]], device='cuda:0')\n",
      "Most likely next token ID: 43787\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787]], device='cuda:0')\n",
      "Most likely next token ID: 5614\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614]], device='cuda:0')\n",
      "Most likely next token ID: 311\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311]], device='cuda:0')\n",
      "Most likely next token ID: 3938\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938]], device='cuda:0')\n",
      "Most likely next token ID: 1473\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473]], device='cuda:0')\n",
      "Most likely next token ID: 8100\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100]],\n",
      "       device='cuda:0')\n",
      "Most likely next token ID: 690\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690]],\n",
      "       device='cuda:0')\n",
      "Most likely next token ID: 1514\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514]],\n",
      "       device='cuda:0')\n",
      "Most likely next token ID: 279\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279]], device='cuda:0')\n",
      "Most likely next token ID: 27374\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374]], device='cuda:0')\n",
      "Most likely next token ID: 32719\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719]], device='cuda:0')\n",
      "Most likely next token ID: 369\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369]], device='cuda:0')\n",
      "Most likely next token ID: 4207\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207]], device='cuda:0')\n",
      "Most likely next token ID: 323\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207,    323]], device='cuda:0')\n",
      "Most likely next token ID: 1243\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207,    323,   1243]],\n",
      "       device='cuda:0')\n",
      "Most likely next token ID: 3009\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207,    323,   1243,   3009]],\n",
      "       device='cuda:0')\n",
      "Most likely next token ID: 439\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207,    323,   1243,   3009,    439]],\n",
      "       device='cuda:0')\n",
      "Most likely next token ID: 433\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207,    323,   1243,   3009,    439,\n",
      "            433]], device='cuda:0')\n",
      "Most likely next token ID: 690\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207,    323,   1243,   3009,    439,\n",
      "            433,    690]], device='cuda:0')\n",
      "Most likely next token ID: 387\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207,    323,   1243,   3009,    439,\n",
      "            433,    690,    387]], device='cuda:0')\n",
      "Most likely next token ID: 33433\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207,    323,   1243,   3009,    439,\n",
      "            433,    690,    387,  33433]], device='cuda:0')\n",
      "Most likely next token ID: 13\n",
      "\n",
      "Input tokens: tensor([[128000, 128000, 128006,   9125, 128007,    271,  29815,    389,    279,\n",
      "           2038,   3984,     11,  18622,    279,  11914,    555,  10223,   1202,\n",
      "          43787,    505,   3347,    311,   3938,     13, 128009, 128006,    882,\n",
      "         128007,    271,   8100,   6476,    279,  27374,  32719,    369,   4207,\n",
      "            323,   1243,  10717,    439,    433,    574,  33433,     13, 128009,\n",
      "         128006,  78191, 128007,    271,   8586,    374,    279,  11914,    449,\n",
      "           1202,  43787,   5614,    311,   3938,   1473,   8100,    690,   1514,\n",
      "            279,  27374,  32719,    369,   4207,    323,   1243,   3009,    439,\n",
      "            433,    690,    387,  33433,     13]], device='cuda:0')\n",
      "Most likely next token ID: 128009\n",
      "\n",
      "Generated text:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Based on the information provided, rewrite the sentence by changing its tense from past to future.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "She played the piano beautifully for hours and then stopped as it was midnight.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here is the sentence with its tense changed to future:\n",
      "\n",
      "She will play the piano beautifully for hours and then stop as it will be midnight.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Use the function\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Based on the information provided, rewrite the sentence by changing its tense from past to future.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "She played the piano beautifully for hours and then stopped as it was midnight.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "generated_ids = generate_tokens(base_model_bnb_4b, tokenizer, prompt, max_new_tokens=50)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "print(f\"\\nGenerated text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee18838-be2d-42b3-86fe-ab89722ce69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
