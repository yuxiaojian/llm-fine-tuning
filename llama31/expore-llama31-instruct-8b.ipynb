{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84c693e-54f7-432f-8c1a-81818039a21c",
   "metadata": {},
   "source": [
    "# [Understand How Llama3.1 Works — A Deep Dive Into the Model Flow](https://medium.com/@yuxiaojian/understand-how-llama3-1-works-a-deep-dive-into-the-model-flow-b149aba04bed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8172ab85-50d7-496b-b044-d918613279e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade transformers\n",
    "#%pip install --upgrade trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b37ec71-ea37-465c-a8a2-5fd61443b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel,PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eecca0fe-a6fe-46a4-9831-77cddd2fa189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "#Configure Huggingface token if download model from Huggingface\n",
    "import getpass\n",
    "import os\n",
    "# Get the Huggingface Token\n",
    "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ecc823e-8c5e-4991-8fe3-e27c26875aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from HF \n",
    "base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835f235f-77b9-4ee3-8944-ee8a393333da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e79fdd03d74d43aeafe3567c8f4653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Quantize the llama3.1 FP16 model to BNB NF4. Load the quantized model to GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Use the code below to load directly from HF\n",
    "base_model_bnb_4b = AutoModelForCausalLM.from_pretrained(base_model, device_map=\"cuda:0\", quantization_config=bnb_config, token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af0c020-2bd2-49d2-b0e0-1151b60c1d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128256\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976bafb-c1ec-4fd2-8feb-5d5e8ed49757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a token and ID mapping\n",
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))\n",
    "\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Based on the information provided, rewrite the sentence by changing its tense from past to future.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "She played the piano beautifully for hours and then stopped as it was midnight.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "print_tokens_with_ids(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c67d87-0d0f-456c-b9dd-a3e054bff9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the token IDs\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934feea3-5eea-418a-91dd-f8d3e9f45b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model\n",
    "outputs = base_model_bnb_4b.generate(input_ids=input_ids,\n",
    "                          pad_token_id=tokenizer.eos_token_id,\n",
    "                          max_new_tokens=200,\n",
    "                          do_sample=True,\n",
    "                          top_p=0.9,\n",
    "                          temperature=0.1)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd447815-2364-4f12-85cd-2bc7e4c80da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the IDs back to tokens\n",
    "result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614fa9a-0382-4841-9ef3-85b0d9e608aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_bnb_4b.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d993c8-29f4-4db0-adc4-9e3a3d982a29",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"image/llama31-arch.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b850cbd-1250-4314-9368-13b3bba90e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_bnb_4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0cb1e0-ac6b-43c3-946c-14d5daa8d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Helper functions for Rotary Position Embedding (RoPE) and grouped-query attention\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    \"\"\"\n",
    "    Apply rotary positional embeddings to query and key tensors.\n",
    "    \n",
    "    Args:\n",
    "    q, k: Query and key tensors\n",
    "    cos, sin: Cosine and sine components of rotary embeddings\n",
    "    position_ids: Tensor of position IDs\n",
    "    \n",
    "    Returns:\n",
    "    q_embed, k_embed: Query and key tensors with rotary embeddings applied\n",
    "    \"\"\"\n",
    "    # Reshape cosine and sine tensors\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    \n",
    "    # Select relevant positional embeddings\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    \n",
    "    # Apply rotary embeddings\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"\n",
    "    Rotate half of the hidden dimensions of the input tensor.\n",
    "    \n",
    "    This operation is part of the rotary position embedding technique.\n",
    "    \n",
    "    Args:\n",
    "    x: Input tensor\n",
    "    \n",
    "    Returns:\n",
    "    Tensor with half of its last dimension rotated\n",
    "    \"\"\"\n",
    "    x1 = x[..., :x.shape[-1] // 2]  # First half of hidden dims\n",
    "    x2 = x[..., x.shape[-1] // 2:]  # Second half of hidden dims\n",
    "    return torch.cat((-x2, x1), dim=-1)  # Concatenate rotated halves\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Repeat key and value states for grouped-query attention.\n",
    "    \n",
    "    This function expands the key and value states to match the number of query heads\n",
    "    in grouped-query attention mechanisms.\n",
    "    \n",
    "    Args:\n",
    "    hidden_states: Input tensor of shape (batch, num_key_value_heads, seqlen, head_dim)\n",
    "    n_rep: Number of repetitions (usually num_query_heads // num_key_value_heads)\n",
    "    \n",
    "    Returns:\n",
    "    Tensor of shape (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states  # No need to repeat if n_rep is 1\n",
    "    \n",
    "    # Expand and reshape to repeat the key/value states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c481e-e85c-4d32-bc7f-18b78321d190",
   "metadata": {},
   "source": [
    "## Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d02ae-4209-4ff0-8bad-d303955ad635",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "position_ids = torch.arange(0, input_ids.shape[1]).unsqueeze(0).cuda()\n",
    "seq_length = input_ids.shape[1]\n",
    "embeddings = base_model_bnb_4b.model.embed_tokens(input_ids)\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7c1ad-56f3-42b8-8e34-d6a94679daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = embeddings\n",
    "hidden_size = base_model_bnb_4b.config.hidden_size\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7b6eb-5300-4cca-a63d-58803e1b5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process through each layer of the model\n",
    "for layer_idx, layer in enumerate(base_model_bnb_4b.model.layers):\n",
    "    print(f\"Processing layer {layer_idx}\")\n",
    "    \n",
    "    # 1. Input LayerNorm\n",
    "    normalized_hidden_states = layer.input_layernorm(hidden_states)\n",
    "    \n",
    "    # 2. Self-attention mechanism\n",
    "    # 2.1 Query, Key, Value projections\n",
    "    query_states = layer.self_attn.q_proj(normalized_hidden_states)\n",
    "    key_states = layer.self_attn.k_proj(normalized_hidden_states)\n",
    "    value_states = layer.self_attn.v_proj(normalized_hidden_states)\n",
    "    \n",
    "    # 2.2 Reshape and transpose Q, K, V\n",
    "    # (batch_size, seq_length, num_heads, head_dim) -> (batch_size, num_heads, seq_length, head_dim)\n",
    "    query_states = query_states.view(batch_size, seq_length, layer.self_attn.num_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(batch_size, seq_length, layer.self_attn.num_key_value_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(batch_size, seq_length, layer.self_attn.num_key_value_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "    \n",
    "    # 2.3 Apply rotary positional embeddings\n",
    "    kv_seq_len = key_states.shape[-2]\n",
    "    cos, sin = layer.self_attn.rotary_emb(value_states, position_ids)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "    \n",
    "    # 2.4 Handle grouped-query attention\n",
    "    # Repeat K and V for each query group\n",
    "    key_states = repeat_kv(key_states, layer.self_attn.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, layer.self_attn.num_key_value_groups)\n",
    "    \n",
    "    # 2.5 Compute attention scores\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(layer.self_attn.head_dim)\n",
    "    attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "    \n",
    "    # 2.6 Apply attention to values\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    \n",
    "    # 2.7 Reshape attention output\n",
    "    # (batch_size, num_heads, seq_length, head_dim) -> (batch_size, seq_length, hidden_size)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, hidden_size)\n",
    "    \n",
    "    # 2.8 Output projection\n",
    "    attn_output = layer.self_attn.o_proj(attn_output)\n",
    "    \n",
    "    # 2.9 Residual connection\n",
    "    hidden_states = hidden_states + attn_output\n",
    "    \n",
    "    # 3. Post-attention LayerNorm\n",
    "    normalized_hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "    \n",
    "    # 4. MLP (Feed-Forward Network)\n",
    "    # 4.1 Apply gate and up projections\n",
    "    gate_output = layer.mlp.gate_proj(normalized_hidden_states)\n",
    "    up_output = layer.mlp.up_proj(normalized_hidden_states)\n",
    "    \n",
    "    # 4.2 Apply activation function and element-wise multiplication\n",
    "    mlp_output = up_output * layer.mlp.act_fn(gate_output)\n",
    "    \n",
    "    # 4.3 Down projection\n",
    "    mlp_output = layer.mlp.down_proj(mlp_output)\n",
    "    \n",
    "    # 4.4 Residual connection\n",
    "    hidden_states = hidden_states + mlp_output\n",
    "    \n",
    "    print(f\"  Output shape: {hidden_states.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e5097-cf7b-4106-bc2a-54a3182f2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final LayerNorm\n",
    "# Normalize the hidden states from the last layer\n",
    "hidden_states = base_model_bnb_4b.model.norm(hidden_states)\n",
    "\n",
    "# Language Model Head\n",
    "# Project the normalized hidden states to the vocabulary space\n",
    "lm_logits = base_model_bnb_4b.lm_head(hidden_states)\n",
    "\n",
    "# Get the logits for the last token\n",
    "# We're only interested in predicting the next token, so we take the last position\n",
    "last_token_logits = lm_logits[:, -1, :]\n",
    "\n",
    "# Note: The following line is commented out as we're using a more sophisticated sampling method\n",
    "# next_token = torch.argmax(last_token_logits, dim=-1)\n",
    "\n",
    "# Step 7: Apply temperature\n",
    "# Temperature adjusts the randomness of predictions. Lower values make the model more confident.\n",
    "temperature = 0.1\n",
    "scaled_logits = last_token_logits / temperature\n",
    "\n",
    "# Step 8: Apply top-p (nucleus) sampling\n",
    "# This method truncates the least likely tokens whose cumulative probability exceeds (1 - top_p)\n",
    "top_p = 0.9\n",
    "# Sort logits in descending order\n",
    "sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
    "# Calculate cumulative probabilities\n",
    "cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "# Remove tokens with cumulative probability above the threshold\n",
    "sorted_indices_to_remove = cumulative_probs > top_p\n",
    "# Shift the indices to keep also the first token above the threshold\n",
    "sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "sorted_indices_to_remove[..., 0] = 0\n",
    "# Scatter sorted tensors to original indexing\n",
    "indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "# Set logits of removed indices to negative infinity\n",
    "scaled_logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "# Step 9: Sample from the filtered distribution\n",
    "# Convert logits to probabilities\n",
    "probs = torch.softmax(scaled_logits, dim=-1)\n",
    "# Randomly sample a token based on the calculated probabilities\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "# Print the input tokens and the predicted next token\n",
    "print(f\"\\nInput tokens: {input_ids}\")\n",
    "print(f\"Most likely next token ID: {next_token.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275b4b6-2d1a-4cd8-ac7c-91d42cfaab5f",
   "metadata": {},
   "source": [
    "## Create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291fd11-0d83-494e-a438-b86e47e1fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, top_k=50, top_p=0.95):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Prepare input\n",
    "        position_ids = torch.arange(0, input_ids.shape[1]).unsqueeze(0).cuda()\n",
    "        seq_length = input_ids.shape[1]\n",
    "        batch_size = 1\n",
    "        hidden_size = model.config.hidden_size\n",
    "\n",
    "        # Token embedding\n",
    "        hidden_states = model.model.embed_tokens(input_ids)\n",
    "\n",
    "        # Process through each layer\n",
    "        for layer in model.model.layers:\n",
    "            # 1. Input LayerNorm\n",
    "            normalized_hidden_states = layer.input_layernorm(hidden_states)\n",
    "            \n",
    "            # 2. Self-attention\n",
    "            query_states = layer.self_attn.q_proj(normalized_hidden_states)\n",
    "            key_states = layer.self_attn.k_proj(normalized_hidden_states)\n",
    "            value_states = layer.self_attn.v_proj(normalized_hidden_states)\n",
    "            \n",
    "            # Reshape and transpose\n",
    "            query_states = query_states.view(batch_size, seq_length, layer.self_attn.num_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "            key_states = key_states.view(batch_size, seq_length, layer.self_attn.num_key_value_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "            value_states = value_states.view(batch_size, seq_length, layer.self_attn.num_key_value_heads, layer.self_attn.head_dim).transpose(1, 2)\n",
    "            \n",
    "            # Apply rotary embeddings\n",
    "            cos, sin = layer.self_attn.rotary_emb(value_states, position_ids)\n",
    "            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "            \n",
    "            # Handle grouped-query attention\n",
    "            key_states = repeat_kv(key_states, layer.self_attn.num_key_value_groups)\n",
    "            value_states = repeat_kv(value_states, layer.self_attn.num_key_value_groups)\n",
    "            \n",
    "            # Compute attention scores and apply attention\n",
    "            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(layer.self_attn.head_dim)\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_states)\n",
    "            \n",
    "            # Reshape attention output\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, hidden_size)\n",
    "            \n",
    "            # Output projection\n",
    "            attn_output = layer.self_attn.o_proj(attn_output)\n",
    "            \n",
    "            # Residual connection\n",
    "            hidden_states = hidden_states + attn_output\n",
    "            \n",
    "            # 3. Post-attention LayerNorm\n",
    "            normalized_hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "            \n",
    "            # 4. MLP\n",
    "            mlp_output = layer.mlp.gate_proj(normalized_hidden_states)\n",
    "            mlp_output = layer.mlp.up_proj(normalized_hidden_states) * layer.mlp.act_fn(mlp_output)\n",
    "            mlp_output = layer.mlp.down_proj(mlp_output)\n",
    "            \n",
    "            # Residual connection\n",
    "            hidden_states = hidden_states + mlp_output\n",
    "\n",
    "        # Final LayerNorm\n",
    "        hidden_states = model.model.norm(hidden_states)\n",
    "\n",
    "        # Language Model Head\n",
    "        lm_logits = model.lm_head(hidden_states)\n",
    "\n",
    "        # Get the logits for the last token\n",
    "        last_token_logits = lm_logits[:, -1, :]\n",
    "\n",
    "\n",
    "        # Step 7: Apply temperature\n",
    "        # Temperature adjusts the randomness of predictions. Lower values make the model more confident.\n",
    "        scaled_logits = last_token_logits / temperature\n",
    "        \n",
    "        # Step 8: Apply top-p (nucleus) sampling\n",
    "        # This method truncates the least likely tokens whose cumulative probability exceeds (1 - top_p)\n",
    "        # Sort logits in descending order\n",
    "        sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
    "        # Calculate cumulative probabilities\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        # Scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        # Set logits of removed indices to negative infinity\n",
    "        scaled_logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Step 9: Sample from the filtered distribution\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.softmax(scaled_logits, dim=-1)\n",
    "        # Randomly sample a token based on the calculated probabilities\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Print the input tokens and the predicted next token\n",
    "        print(f\"\\nInput tokens: {input_ids}\")\n",
    "        print(f\"Most likely next token ID: {next_token.item()}\")\n",
    "\n",
    "        # Append the new token to input_ids\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        \n",
    "        # Check if we've generated an end-of-sequence token\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc24a4-69b1-410c-9bea-ac4c07c0cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Based on the information provided, rewrite the sentence by changing its tense from past to future.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "She played the piano beautifully for hours and then stopped as it was midnight.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "generated_ids = generate_tokens(base_model_bnb_4b, tokenizer, prompt, max_new_tokens=50)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "print(f\"\\nGenerated text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee18838-be2d-42b3-86fe-ab89722ce69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
